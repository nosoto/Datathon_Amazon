{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18727654-3f0a-483c-832b-e5f699ff7ad9",
      "metadata": {
        "id": "18727654-3f0a-483c-832b-e5f699ff7ad9"
      },
      "source": [
        "# Datathon - Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b415cf8-6a2d-439f-85a7-c530e718263d",
      "metadata": {
        "id": "0b415cf8-6a2d-439f-85a7-c530e718263d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pandas_profiling import ProfileReport\n",
        "from functools import partial\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.cluster import KMeans\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333d4a85-6f89-4c50-9d40-97f6602b0a9f",
      "metadata": {
        "id": "333d4a85-6f89-4c50-9d40-97f6602b0a9f"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb6505e-16f1-4243-b26f-843a713358f7",
      "metadata": {
        "id": "4bb6505e-16f1-4243-b26f-843a713358f7"
      },
      "source": [
        "### date_vars_create\n",
        "- **Inputs:**\n",
        "    - **df:** DataFrame to modify\n",
        "    - **base_date_column:** source date column of the dataset (in our case: ofd_date)\n",
        "    - **columns_to_create:** list of new date columns we want to create. Options are: \n",
        "        - weekday (e.g. 'Monday', 'Tuesday', etc.) <- dtype: object\n",
        "        - month (e.g. 'January', 'February', etc.) <- dtype: object\n",
        "        - day_of_month (1, 2, 3, 4, etc.) <- dtype: int64\n",
        "- **Output:**\n",
        "    - Modified DF with the additional chosen date columns (columns_to_create)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5d0bd2-199a-4b51-9ad5-1bee4a8f88f0",
      "metadata": {
        "id": "0e5d0bd2-199a-4b51-9ad5-1bee4a8f88f0"
      },
      "outputs": [],
      "source": [
        "# Change Date Vars based on column names\n",
        "def date_vars_create(df, base_date_column, columns_to_create):\n",
        "    df[base_date_column] = pd.to_datetime(df[base_date_column], format='%Y-%m-%d')\n",
        "    df.sort_values(by=[base_date_column, 'station_code'], inplace=True)\n",
        "    \n",
        "    for column in columns_to_create:\n",
        "        if 'weekday' == column:\n",
        "            df[column] = df[base_date_column].dt.day_name()\n",
        "        if 'month' == column:\n",
        "            df[column] = df[base_date_column].dt.month_name()\n",
        "        if 'day_of_month' == column:\n",
        "            df[column] = df[base_date_column].dt.day\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8a6727e-bbc2-4a11-8da9-561b2323e777",
      "metadata": {
        "id": "c8a6727e-bbc2-4a11-8da9-561b2323e777"
      },
      "source": [
        "### Clustering Station Codes\n",
        "- Clusters station codes to reduce cardinality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d5a0056-67f5-44d6-bfcd-704635b2e2bc",
      "metadata": {
        "id": "8d5a0056-67f5-44d6-bfcd-704635b2e2bc"
      },
      "outputs": [],
      "source": [
        "def conversion_function(value):\n",
        "        return dict[value]\n",
        "\n",
        "def encode_and_cluster_stations(df, mean_var_list, cluster_list, groupby_var, n_clusters):\n",
        "    # Encode based on means\n",
        "    df_grouped = pd.DataFrame(df.groupby(by=[groupby_var], as_index = False).mean())\n",
        "    #df_grouped = df_grouped.drop(columns = ['country_code'])\n",
        "    \n",
        "    count = 0\n",
        "    for variable in mean_var_list:\n",
        "        count += 1 \n",
        "        mean_var = df_grouped.groupby(groupby_var)[variable].mean()\n",
        "        df_grouped.loc[:, ('en_var' + str(count))] = df_grouped[groupby_var].map(mean_var)\n",
        "    \n",
        "    df_grouped['mean_encode'] = df_grouped[mean_var_list[0]] + df_grouped[mean_var_list[1]]\n",
        "    \n",
        "    df_grouped.head\n",
        "\n",
        "    # Create Cluster\n",
        "    df_cluster = df_grouped[ cluster_list + ['mean_encode']]\n",
        "    \n",
        "    kmean = KMeans(n_clusters=n_clusters, random_state=0).fit(df_cluster)\n",
        "    \n",
        "    # Map DC to New Clusters\n",
        "    df_mapping = list(kmean.predict(df_cluster))\n",
        "    conversion_dict = {}\n",
        "    \n",
        "    for i in range(0, len(df_mapping)):\n",
        "        conversion_dict[df_grouped[groupby_var][i]] = df_mapping[i]\n",
        "\n",
        "    df['DC'] = df[groupby_var].apply(lambda x: conversion_dict[x])\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7407f752-cc87-467e-980e-b59b604775f1",
      "metadata": {
        "id": "7407f752-cc87-467e-980e-b59b604775f1"
      },
      "source": [
        "###Â Encode Variables\n",
        "- **Inputs**:\n",
        "    - **df**: DataFrame to modify,\n",
        "    - **var_list**: List of variables (columns) to encode. \n",
        "- Note: If fc_codes is in the list of columns to encode, it will be treated separately (i.e. not using pd.get_dummies)\n",
        "- **Output**:\n",
        "    - Modified DF with all dummy columns (!Original cols will be dropped!) <- dtypes: uint8\n",
        "\n",
        "### Encode FC\n",
        "- **Inputs**:\n",
        "    - **df**: DataFrame to modify,\n",
        "    - **col_name**: Name of the column to do special encode. (It's only fc_code, but doing this to be more flexible)\n",
        "- **Output**:\n",
        "    - Modified DF with dummy columns created for every fc_code (they are separated by ', ' first)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84f4f32-cbe3-4c0d-8b2d-05c2ab8a46a4",
      "metadata": {
        "id": "c84f4f32-cbe3-4c0d-8b2d-05c2ab8a46a4"
      },
      "outputs": [],
      "source": [
        "def vars_encode(df, var_list):\n",
        "    \n",
        "    if 'fc_codes' in var_list:\n",
        "        var_list = [x for x in var_list if x != 'fc_codes']\n",
        "        df = encode_fc(df, 'fc_codes')\n",
        "    \n",
        "    prefix_list = ['DoM' if x == 'day_of_month' else x for x in var_list]\n",
        "    new_data = pd.get_dummies(df, columns=var_list, prefix=prefix_list, prefix_sep='_')\n",
        "    \n",
        "    return new_data\n",
        "\n",
        "\n",
        "def encode_fc(df, col_name):\n",
        "    vals = list(df[col_name].str.split(', ').values)\n",
        "    vals = [i for l in vals for i in l]\n",
        "    vals = list(set(vals))\n",
        "    vals.sort()\n",
        "\n",
        "    for v in vals:\n",
        "        n = col_name + '_' + v\n",
        "        df[n] = df[col_name].str.contains(v)\n",
        "        df[n] = df[n].astype('uint8')\n",
        "    df.drop(columns=[col_name], inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "981ea543-9da6-4242-ad15-dc0457add89d",
      "metadata": {
        "id": "981ea543-9da6-4242-ad15-dc0457add89d"
      },
      "source": [
        "### Outlier Management\n",
        "- Inputs:\n",
        "    - df: DataFrame to modify,\n",
        "    - column_list: List of **numeric** columns to handle outliers for. \n",
        "    - perc_limit = Outlier Threshold. Should be symmetric (i.e. top x% and bottom x% are winsorized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df35aa26-658a-49ba-8c1e-c27054dba572",
      "metadata": {
        "id": "df35aa26-658a-49ba-8c1e-c27054dba572"
      },
      "outputs": [],
      "source": [
        "def outlier_management(df, column_list, perc_limit=0.025):\n",
        "    for column in column_list:\n",
        "        df[column] = winsorize(df[column], limits=(perc_limit, perc_limit))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f1b2d5-c1e2-40dc-a066-01ddbf1b97b3",
      "metadata": {
        "id": "50f1b2d5-c1e2-40dc-a066-01ddbf1b97b3"
      },
      "source": [
        "### addSimpleLags_Diffs\n",
        "-> This function uses a partial function (_buildLags_Diffs_) to process individual lags. We're essentially bulk applying a single lag (using map) to all columns at once).\n",
        "\n",
        "- **Inputs**:\n",
        "    - **df**: DataFrame to modify, \n",
        "    - **lag_list**: list of periods of lags or diffs to do (ex. 1 = lag by 1 row, or subtract previous row on current row),\n",
        "    - **col_list**: list of columns to apply the lagging/diff(ing) to,\n",
        "    - **change_choice**: either lag, lead, or diff (diff will by default be backwards diff).\n",
        "- **Output**:\n",
        "    - Modified DF <- dtypes: int64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29cb35b2-9000-4ef4-9cdb-c033745004bb",
      "metadata": {
        "id": "29cb35b2-9000-4ef4-9cdb-c033745004bb"
      },
      "outputs": [],
      "source": [
        "# Creating Lags\n",
        "\n",
        "def addSimpleLags_Diffs(df,lag_list, col_list, change_choice = 'lag'):\n",
        "\n",
        "    if change_choice == 'lead':\n",
        "        lag_list = map(lambda x: x*(-1),lag_list)\n",
        "\n",
        "    arr_lags = list(map(partial(_buildLags_Diffs,df=df,\n",
        "                        col_list=col_list,\n",
        "                        change_choice = change_choice),\n",
        "                        lag_list))\n",
        "\n",
        "    df = pd.concat([df]+arr_lags,axis = 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def _buildLags_Diffs(lag,df,col_list, change_choice):\n",
        "    if change_choice == 'lag' or change_choice == 'lead':\n",
        "        return df.groupby('station_code')[col_list].shift(lag).add_suffix(f'_{np.abs(lag)}_{change_choice}')\n",
        "    elif change_choice == 'diff':\n",
        "        return df.groupby('station_code')[col_list].diff(lag).add_suffix(f'_{np.abs(lag)}_{change_choice}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b026dd4f-dcce-4d53-8386-c82e433ccfcd",
      "metadata": {
        "id": "b026dd4f-dcce-4d53-8386-c82e433ccfcd"
      },
      "source": [
        "### data_processor\n",
        "**-> Acts like the __main__ function**\n",
        "\n",
        "- **Inputs**:\n",
        "    - **df:** DataFrame to modify,\n",
        "    - **dep_vars:** list of target variables (that should not be processed but left alone.),\n",
        "    - **date_col:** base date column to use for date variable creation,\n",
        "    - **date_cols_ToDo:** choice of different date columns we want to create (**see date_vars_create**),\n",
        "    - **lag_diff_ToDo:** choice of different lags/diff periods to do (**see addSimpleLags_Diffs**),\n",
        "    - **lagChoice:** default is both (does both Lag & Diff). Otherwise will only do lag, diff, or lead. \n",
        "    - **verbose:** Bool to choose whether you want to list out all the columns created or not. \n",
        "- **Output**:\n",
        "    - Final modified DF to use for Splitting + Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "561191e6-d202-46e6-a11e-bb1838629e8d",
      "metadata": {
        "id": "561191e6-d202-46e6-a11e-bb1838629e8d"
      },
      "outputs": [],
      "source": [
        "def data_processor(df, dep_vars, date_col, date_cols_ToDo, univariate_flag=False, lag_ToDo=[1, 2, 4, 5, 6, 7], diff_ToDo=None, mean_vars_clustering=['OFD', 'Slam'], cluster_groupby='station_code', n_clusters=7, \n",
        "                   outlier_threshold=0.025, lagChoice='both', verbose=False):\n",
        "    \n",
        "    if univariate_flag:\n",
        "        df['Target'] = df['Earlies_Exp'] - df['MNR_SNR_Exp']\n",
        "        # All int64, including target variables.\n",
        "        LagDiff_vars_ToDo = [x for x in list(df.select_dtypes(['int64']).columns)]\n",
        "        \n",
        "    elif not univariate_flag:\n",
        "        LagDiff_vars_ToDo = [x for x in list(df.select_dtypes(['int64']).columns) if x not in dep_vars]\n",
        "        \n",
        "\n",
        "   \n",
        "    # All 'object' dtype columns from the original DF (excluding the date column) should be turned into dummies\n",
        "    encode_basevars_ToDo = [x for x in list(df.select_dtypes(['object']).columns) if x != date_col]\n",
        "    encode_basevars_ToDo.append('DC')\n",
        "    \n",
        "    # Manage outliers\n",
        "    df_mod = outlier_management(df, LagDiff_vars_ToDo, outlier_threshold)\n",
        "    \n",
        "    final_encode_list = encode_basevars_ToDo + date_cols_ToDo\n",
        "    \n",
        "    # Create Date Variables\n",
        "    df_mod = date_vars_create(df_mod, date_col, date_cols_ToDo)\n",
        "    \n",
        "    if lagChoice == 'both':\n",
        "        df_mod = addSimpleLags_Diffs(df_mod, lag_ToDo, LagDiff_vars_ToDo, change_choice='lag')\n",
        "        df_mod = addSimpleLags_Diffs(df_mod, diff_ToDo, LagDiff_vars_ToDo, change_choice='diff')\n",
        "    \n",
        "    elif lagChoice in ['lag', 'diff', 'lead']:\n",
        "        df_mod = addSimpleLags_Diffs(df_mod, lag_ToDo, LagDiff_vars_ToDo, change_choice=lagChoice)\n",
        "        \n",
        "    df_mod = df_mod.fillna(0)\n",
        "    \n",
        "    \n",
        "    df_mod = encode_and_cluster_stations(df_mod, mean_var_list=mean_vars_clustering, cluster_list=LagDiff_vars_ToDo + dep_vars, groupby_var=cluster_groupby,  n_clusters=n_clusters)\n",
        "    # Encode columns. DC should be excluded from this as we do a special encoding. \n",
        "    df_mod = vars_encode(df_mod, final_encode_list)\n",
        "    \n",
        "    \n",
        "    if verbose:\n",
        "        for column in list(df_mod.columns):\n",
        "            print(column)\n",
        "    else:\n",
        "        print(\"Data processing is done.\")\n",
        "        print(f\"We now have {len(list(df_mod.columns))} features. Woop woop.\")\n",
        "            \n",
        "    return df_mod"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca0cc726-3858-4293-9690-1b70372b2672",
      "metadata": {
        "id": "ca0cc726-3858-4293-9690-1b70372b2672"
      },
      "source": [
        "# Actual Processing of Data\n",
        "- Load Train Data from CSV.\n",
        "- Define base parameters\n",
        "- Run data processing.\n",
        "\n",
        "- You can optionally toggle verbose arg to list out cols or not.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c341633e-407c-4cf0-a20f-772933b3eb91",
      "metadata": {
        "id": "c341633e-407c-4cf0-a20f-772933b3eb91"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "train_data = pd.read_csv('train_data.csv', sep=',')\n",
        "train_data.dtypes\n",
        "train_data.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a2d81d-ab27-4770-93e9-2a5c1032a8d8",
      "metadata": {
        "id": "e9a2d81d-ab27-4770-93e9-2a5c1032a8d8"
      },
      "outputs": [],
      "source": [
        "# Define Base Parameters\n",
        "target_vars = ['Earlies_Exp', 'MNR_SNR_Exp']\n",
        "base_date_col = 'ofd_date'\n",
        "date_columns_to_create = ['weekday', 'month', 'day_of_month']\n",
        "\n",
        "lag_to_do = []\n",
        "for x in range(1, 31):\n",
        "    lag_to_do.append(x)\n",
        "\n",
        "diff_to_do = [1, 2, 3, 7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b914ebed-3129-48d4-8fec-c22abe55f6d5",
      "metadata": {
        "id": "b914ebed-3129-48d4-8fec-c22abe55f6d5",
        "outputId": "33779ec2-513d-4caf-c40a-7104bd965159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data processing is done.\n",
            "We now have 437 features. Woop woop.\n"
          ]
        }
      ],
      "source": [
        "# Run data processor\n",
        "final_data = data_processor(df=train_data, \n",
        "                            dep_vars=target_vars, \n",
        "                            date_col=base_date_col, \n",
        "                            date_cols_ToDo=date_columns_to_create,\n",
        "                            univariate_flag=False,\n",
        "                            lag_ToDo=lag_to_do,\n",
        "                            diff_ToDo = diff_to_do,\n",
        "                            mean_vars_clustering=['OFD', 'Slam'],\n",
        "                            cluster_groupby='station_code',\n",
        "                            n_clusters=7,\n",
        "                            outlier_threshold=0.025,\n",
        "                            lagChoice='both',\n",
        "                            verbose=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DEAvenv",
      "language": "python",
      "name": "deavenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "DataTuna_Data_Processor_Full.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}